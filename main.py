# File: main.py
import os
import json
import pickle
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from flask import Flask, request, jsonify
from flask_cors import CORS
from google.cloud import storage
# TH√äM M·ªöI ƒê·ªÇ S·ª¨A L·ªñI
from google.oauth2 import service_account

print("--- [INFO] B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o ·ª©ng d·ª•ng ---")

# --- ·ª®ng d·ª•ng Flask ---
app = Flask(__name__)
CORS(app)

# --- C·∫•u h√¨nh ---
MAX_LEN = 2000
MODEL_LOCAL_PATH = "transformer_xs_model.h5"
TOKENIZER_LOCAL_PATH = "tokenizer.pickle"
MODEL_GCS_PATH = "models/transformer_v1"

# --- Thi·∫øt l·∫≠p GCS ---
gcs_bucket = None
try:
    GCS_BUCKET_NAME = os.environ.get('GCS_BUCKET_NAME')
    GCS_CREDENTIALS_JSON = os.environ.get('GCS_CREDENTIALS')
    if GCS_BUCKET_NAME and GCS_CREDENTIALS_JSON:
        credentials_dict = json.loads(GCS_CREDENTIALS_JSON)
        
        # S·ª¨A L·ªñI K·∫æT N·ªêI GCS
        credentials = service_account.Credentials.from_service_account_info(credentials_dict)
        storage_client = storage.Client(credentials=credentials)
        
        gcs_bucket = storage_client.bucket(GCS_BUCKET_NAME)
        print(f"‚úÖ [GCS] K·∫øt n·ªëi th√†nh c√¥ng ƒë·∫øn bucket: {GCS_BUCKET_NAME}")
    else:
        print("‚ö†Ô∏è [GCS] Bi·∫øn m√¥i tr∆∞·ªùng GCS ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p. S·∫Ω ch·ªâ d√πng model local.")
except Exception as e:
    print(f"‚ùå [GCS] L·ªói kh·ªüi t·∫°o GCS: {e}")

# --- C√°c ph·∫ßn c√≤n l·∫°i c·ªßa file gi·ªØ nguy√™n kh√¥ng ƒë·ªïi ---

# --- ƒê·ªãnh nghƒ©a L·ªõp Custom ---
class PositionalEmbedding(layers.Layer):
    def __init__(self, vocab_size, embed_dim, maxlen, **kwargs):
        super().__init__(**kwargs)
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)
        self.maxlen = maxlen
    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

class TransformerEncoder(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation="relu"), layers.Dense(embed_dim),])
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
    def call(self, inputs, mask=None):
        attention_output = self.attention(inputs, inputs, attention_mask=mask)
        proj_input = self.layernorm_1(inputs + attention_output)
        proj_output = self.dense_proj(proj_input)
        return self.layernorm_2(proj_input + proj_output)

custom_objects = {
    "PositionalEmbedding": PositionalEmbedding,
    "TransformerEncoder": TransformerEncoder
}

# --- H√†m T·∫£i/L∆∞u Model ---
def load_model_from_gcs():
    if not gcs_bucket: return None
    try:
        print("üîç [GCS] ƒêang th·ª≠ t·∫£i model t·ª´ GCS...")
        gcs_path = f"gs://{GCS_BUCKET_NAME}/{MODEL_GCS_PATH}"
        loaded_model = tf.keras.models.load_model(gcs_path, custom_objects=custom_objects)
        print("‚úÖ [GCS] T·∫£i model t·ª´ GCS th√†nh c√¥ng!")
        return loaded_model
    except Exception as e:
        if "NotFoundError" in str(e) or "doesn't exist" in str(e):
             print(f"‚ÑπÔ∏è [GCS] Kh√¥ng t√¨m th·∫•y model tr√™n GCS. ƒê√¢y c√≥ th·ªÉ l√† l·∫ßn deploy ƒë·∫ßu ti√™n.")
        else:
             print(f"‚ö†Ô∏è [GCS] L·ªói kh√°c khi t·∫£i model t·ª´ GCS: {e}")
        return None

def load_model_from_local():
    try:
        if not os.path.exists(MODEL_LOCAL_PATH):
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file model local t·∫°i: {MODEL_LOCAL_PATH}")
            return None
        print("üíæ ƒêang t·∫£i model d·ª± ph√≤ng t·ª´ file local...")
        loaded_model = tf.keras.models.load_model(MODEL_LOCAL_PATH, custom_objects=custom_objects)
        print("‚úÖ T·∫£i model local th√†nh c√¥ng.")
        return loaded_model
    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ t·∫£i model local: {e}")
        return None

def save_model_to_gcs(model_to_save):
    if not gcs_bucket:
        print("‚ö†Ô∏è [GCS] Kh√¥ng th·ªÉ l∆∞u model v√¨ GCS ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.")
        return False
    try:
        print("üíæ [GCS] ƒêang l∆∞u model m·ªõi l√™n GCS...")
        gcs_path = f"gs://{GCS_BUCKET_NAME}/{MODEL_GCS_PATH}"
        model_to_save.save(gcs_path)
        print("‚úÖ [GCS] L∆∞u model l√™n GCS th√†nh c√¥ng!")
        return True
    except Exception as e:
        print(f"‚ùå [GCS] L·ªói khi l∆∞u model l√™n GCS: {e}")
        return False

# --- Kh·ªüi t·∫°o Bi·∫øn To√†n c·ª•c ---
print("--- [INFO] ƒêang t·∫£i tokenizer v√† model ---")
model = None
tokenizer = None
try:
    with open(TOKENIZER_LOCAL_PATH, 'rb') as handle:
        tokenizer = pickle.load(handle)
        print("‚úÖ T·∫£i tokenizer th√†nh c√¥ng.")
except Exception as e:
    print(f"CRITICAL ERROR: Kh√¥ng th·ªÉ t·∫£i tokenizer: {e}")
model = load_model_from_gcs()
if model is None:
    model = load_model_from_local()
if model is None:
    print("CRITICAL ERROR: Kh√¥ng th·ªÉ t·∫£i ƒë∆∞·ª£c b·∫•t k·ª≥ model n√†o. API '/predict' v√† '/learn' s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
else:
    print("--- [INFO] Model ƒë√£ s·∫µn s√†ng ---")

# --- API Endpoints ---
@app.route('/predict', methods=['POST'])
def predict():
    if model is None or tokenizer is None:
        return jsonify({'success': False, 'message': 'Model ho·∫∑c tokenizer ch∆∞a ƒë∆∞·ª£c t·∫£i.'}), 503
    try:
        history_results = request.json['history']
        df_hist = pd.DataFrame(history_results)
        df_hist['so_str'] = df_hist['so'].astype(str)
        input_text = ''.join(df_hist['so_str'].tolist())
        seq = tokenizer.texts_to_sequences([input_text])
        padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=MAX_LEN, padding='post')
        predictions = model.predict(padded_seq)
        result = {
            'hangChucNgan': int(np.argmax(predictions[0])),
            'hangNgan': int(np.argmax(predictions[1])),
            'hangTram': int(np.argmax(predictions[2])),
            'hangChuc': int(np.argmax(predictions[3])),
            'hangDonVi': int(np.argmax(predictions[4])),
        }
        return jsonify({'success': True, 'prediction': result})
    except Exception as e:
        return jsonify({'success': False, 'message': f"L·ªói khi d·ª± ƒëo√°n: {e}"})

@app.route('/learn', methods=['POST'])
def learn():
    global model
    if model is None:
        return jsonify({'success': False, 'message': 'Model ch∆∞a ƒë∆∞·ª£c t·∫£i.'}), 503
    try:
        training_sample = request.json['sample']
        input_seq = training_sample['input']
        target_gdb = training_sample['output']
        input_pad = tf.keras.preprocessing.sequence.pad_sequences([input_seq], maxlen=MAX_LEN, padding='post')
        y_split = [np.array([d]) for d in ta
