# File: app.py
from flask import Flask, request, jsonify
from flask_cors import CORS
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import pickle
import pandas as pd
import os
import json

# TH√äM M·ªöI: Th∆∞ vi·ªán Google Cloud Storage
from google.cloud import storage

app = Flask(__name__)
CORS(app)

# --- C·∫§U H√åNH ---
MAX_LEN = 2000 # Ph·∫£i gi·ªëng h·ªát l√∫c train
MODEL_LOCAL_PATH = "transformer_xs_model.h5"
TOKENIZER_LOCAL_PATH = "tokenizer.pickle"
MODEL_GCS_PATH = "models/transformer_v1" # ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c tr√™n GCS ƒë·ªÉ l∆∞u model

# --- THI·∫æT L·∫¨P K·∫æT N·ªêI GCS ---
GCS_BUCKET_NAME = os.environ.get('GCS_BUCKET_NAME')
GCS_CREDENTIALS_JSON = os.environ.get('GCS_CREDENTIALS')
gcs_bucket = None

if GCS_BUCKET_NAME and GCS_CREDENTIALS_JSON:
    try:
        credentials_dict = json.loads(GCS_CREDENTIALS_JSON)
        storage_client = storage.Client(credentials=storage.credentials.Credentials.from_service_account_info(credentials_dict))
        gcs_bucket = storage_client.bucket(GCS_BUCKET_NAME)
        print(f"‚úÖ [GCS] K·∫øt n·ªëi th√†nh c√¥ng ƒë·∫øn bucket: {GCS_BUCKET_NAME}")
    except Exception as e:
        print(f"‚ùå [GCS] L·ªói k·∫øt n·ªëi GCS: {e}")
else:
    print("‚ö†Ô∏è [GCS] Bi·∫øn m√¥i tr∆∞·ªùng GCS ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p. S·∫Ω ch·ªâ d√πng model local.")

# =================================================================
# B∆Ø·ªöC 1: ƒê·ªäNH NGHƒ®A L·∫†I C√ÅC L·ªöP CUSTOM
# Copy y h·ªát c√°c class n√†y t·ª´ file train_transformer.py sang ƒë√¢y
# =================================================================
class PositionalEmbedding(layers.Layer):
    def __init__(self, vocab_size, embed_dim, maxlen, **kwargs):
        super().__init__(**kwargs)
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)
        self.maxlen = maxlen
    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

class TransformerEncoder(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation="relu"), layers.Dense(embed_dim),])
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
    def call(self, inputs, mask=None):
        attention_output = self.attention(inputs, inputs, attention_mask=mask)
        proj_input = self.layernorm_1(inputs + attention_output)
        proj_output = self.dense_proj(proj_input)
        return self.layernorm_2(proj_input + proj_output)
# =================================================================
def load_model_from_gcs():
    """C·ªë g·∫Øng t·∫£i model t·ª´ GCS."""
    if not gcs_bucket:
        return None
    try:
        print("üîç [GCS] ƒêang th·ª≠ t·∫£i model t·ª´ GCS...")
        # Keras c·∫ßn l∆∞u/t·∫£i t·ª´ m·ªôt ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c, GCS IO handler s·∫Ω x·ª≠ l√Ω vi·ªác n√†y
        gcs_path = f"gs://{GCS_BUCKET_NAME}/{MODEL_GCS_PATH}"
        model = tf.keras.models.load_model(
            gcs_path,
            custom_objects={
                "PositionalEmbedding": PositionalEmbedding,
                "TransformerEncoder": TransformerEncoder
            }
        )
        print("‚úÖ [GCS] T·∫£i model t·ª´ GCS th√†nh c√¥ng!")
        return model
    except Exception as e:
        print(f"‚ö†Ô∏è [GCS] Kh√¥ng t√¨m th·∫•y model tr√™n GCS ho·∫∑c c√≥ l·ªói: {e}. S·∫Ω d√πng model local.")
        return None

def load_model_from_local():
    """T·∫£i model d·ª± ph√≤ng t·ª´ file local."""
    print("üíæ ƒêang t·∫£i model d·ª± ph√≤ng t·ª´ file local...")
    return tf.keras.models.load_model(
        MODEL_LOCAL_PATH,
        custom_objects={
            "PositionalEmbedding": PositionalEmbedding,
            "TransformerEncoder": TransformerEncoder
        }
    )

def save_model_to_gcs(model_to_save):
    """L∆∞u model l√™n GCS."""
    if not gcs_bucket:
        print("‚ö†Ô∏è [GCS] Kh√¥ng th·ªÉ l∆∞u model v√¨ GCS ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.")
        return False
    try:
        print("üíæ [GCS] ƒêang l∆∞u model m·ªõi l√™n GCS...")
        gcs_path = f"gs://{GCS_BUCKET_NAME}/{MODEL_GCS_PATH}"
        model_to_save.save(gcs_path)
        print("‚úÖ [GCS] L∆∞u model l√™n GCS th√†nh c√¥ng!")
        return True
    except Exception as e:
        print(f"‚ùå [GCS] L·ªói khi l∆∞u model l√™n GCS: {e}")
        return False

# --- KH·ªûI T·∫†O BI·∫æN TO√ÄN C·ª§C ---
# T·∫£i tokenizer
with open(TOKENIZER_LOCAL_PATH, 'rb') as handle:
    tokenizer = pickle.load(handle)

# T·∫£i model (∆Øu ti√™n GCS, n·∫øu th·∫•t b·∫°i th√¨ d√πng local)
model = load_model_from_gcs()
if model is None:
    model = load_model_from_local()


# --- ƒê·ªäNH NGHƒ®A C√ÅC API ENDPOINT ---

@app.route('/predict', methods=['POST'])
def predict():
    # Logic predict kh√¥ng thay ƒë·ªïi
    try:
        history_results = request.json['history']
        # ... (to√†n b·ªô logic ti·ªÅn x·ª≠ l√Ω v√† d·ª± ƒëo√°n gi·ªØ nguy√™n)
        df_hist = pd.DataFrame(history_results)
        df_hist['so_str'] = df_hist['so'].astype(str)
        input_text = ''.join(df_hist['so_str'].tolist())
        seq = tokenizer.texts_to_sequences([input_text])
        padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=MAX_LEN, padding='post')
        predictions = model.predict(padded_seq)
        result = {
            'hangChucNgan': int(np.argmax(predictions[0])),
            'hangNgan': int(np.argmax(predictions[1])),
            'hangTram': int(np.argmax(predictions[2])),
            'hangChuc': int(np.argmax(predictions[3])),
            'hangDonVi': int(np.argmax(predictions[4])),
        }
        return jsonify({'success': True, 'prediction': result})
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})

@app.route('/learn', methods=['POST'])
def learn():
    """API m·ªõi ƒë·ªÉ th·ª±c hi·ªán h·ªçc th√™m (fine-tuning)."""
    global model # Khai b√°o ƒë·ªÉ c√≥ th·ªÉ g√°n l·∫°i model m·ªõi
    try:
        # 1. L·∫•y d·ªØ li·ªáu h·ªçc t·ª´ request
        training_sample = request.json['sample'] # sample = { 'input': [...], 'output': [...] }
        input_seq = training_sample['input']
        target_gdb = training_sample['output']

        # 2. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (gi·ªëng l√∫c train)
        input_pad = tf.keras.preprocessing.sequence.pad_sequences([input_seq], maxlen=MAX_LEN, padding='post')
        y_split = [np.array([d]) for d in target_gdb] # Chuy·ªÉn th√†nh d·∫°ng batch size 1

        # 3. H·ªçc th√™m v·ªõi learning rate nh·ªè
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # learning rate r·∫•t nh·ªè
            loss="sparse_categorical_crossentropy",
            metrics=["accuracy"]
        )
        model.fit(input_pad, y_split, epochs=3, verbose=0) # Ch·ªâ h·ªçc v√†i epoch

        print("üß† Model ƒë√£ h·ªçc th√™m t·ª´ d·ªØ li·ªáu m·ªõi.")

        # 4. L∆∞u l·∫°i model ƒë√£ "th√¥ng minh" h∆°n l√™n GCS
        save_model_to_gcs(model)

        return jsonify({'success': True, 'message': 'Model learned and updated successfully.'})

    except Exception as e:
        return jsonify({'success': False, 'message': f"Error during learning: {e}"})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001)
